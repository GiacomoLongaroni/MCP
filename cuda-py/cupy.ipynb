{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy: A GPU-Accelerated Drop-In Replacement for NumPy\n",
    "\n",
    "CuPy is a library that mirrors NumPy's functionality but utilizes the GPU for computations, offering an almost seamless way to accelerate Python code.\n",
    "\n",
    "Like NumPy, CuPy provides three core components:\n",
    "\n",
    "1. A multidimensional array object (stored in GPU memory)\n",
    "2. A `ufunc` system that follows broadcasting rules (executing operations in parallel on the GPU)\n",
    "3. A comprehensive library of array functions (implemented with CUDA for efficient GPU execution)\n",
    "\n",
    "One of CuPy's main strengths is its ability to serve as a drop-in replacement for NumPy, allowing us to write \"agnostic code\" that can run on either CPU or GPU, depending on the available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations with CuPy (Comparing with NumPy)\n",
    "\n",
    "Let's start with a few basic operations using CuPy, similar to how we would with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector sum with NumPy\n",
    "x_cpu = np.linspace(0, 100, 20)\n",
    "y_cpu = np.linspace(10, 200, 20)\n",
    "z_cpu = x_cpu + y_cpu\n",
    "z_cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-matrix multiplication with NumPy (CPU)\n",
    "x_cpu = np.random.random(1_000).reshape(20, 50)\n",
    "y_cpu = np.random.random(1_000).reshape(50, 20)\n",
    "z_cpu = np.dot(x_cpu, y_cpu)\n",
    "z_cpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the same operations with CuPy, we simply need to change the library import from `numpy` to `cupy`. This allows us to run the operations on the GPU instead of the CPU, with minimal code modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector sum on the GPU using CuPy\n",
    "x_gpu = cp.linspace(0, 100, 20)\n",
    "y_gpu = cp.linspace(10, 200, 20)\n",
    "z_gpu = x_gpu + y_gpu\n",
    "z_gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-matrix multiplication on the GPU using CuPy\n",
    "x_gpu = cp.random.random(1_000).reshape(20, 50)\n",
    "y_gpu = cp.random.random(1_000).reshape(50, 20)\n",
    "z_gpu = cp.dot(x_gpu, y_gpu)\n",
    "z_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy's `numpy.ndarray` objects, CuPy arrays are represented as `cupy.ndarray`. \n",
    "\n",
    "By default, CuPy _infers_ the data type for you, but it may not always choose the optimal format for your specific use case. It's a good practice to explicitly define the data type when necessary to ensure optimal performance and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the type, data type, and shape of the NumPy array\n",
    "print(type(z_cpu))     # Type of the array\n",
    "print(z_cpu.dtype)     # Data type of the elements in the array\n",
    "print(z_cpu.shape)     # Shape of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the type, data type, shape, and device of the CuPy array\n",
    "print(type(z_gpu))       # Type of the array\n",
    "print(z_gpu.dtype)       # Data type of the elements in the array\n",
    "print(z_gpu.shape)       # Shape of the array\n",
    "print(z_gpu.device)      # Device where the array is allocated (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All operations executed with CuPy incur some overhead in terms of execution time. This overhead arises from the time required to compile the code into CUDA and transfer it for execution on the GPU.\n",
    "\n",
    "While memory management in CuPy is handled automatically, it may not always perform as efficiently as desired. However, you have the ability to control data transfers to and from the GPU as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NumPy array on the host and transfer it to the device (GPU)\n",
    "a_cpu = np.array([0, 1, 2, 3, 4, 5])\n",
    "a_gpu = cp.asarray(a_cpu)  # Transfer to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a computation on the CuPy array on the device (GPU)\n",
    "b_gpu = cp.exp(a_gpu.reshape(2, 3))  # Calculate the exponential of each element after reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the device data back to the host (CPU)\n",
    "b_cpu = b_gpu.get()  # Transfer the data from GPU to CPU\n",
    "b_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agnostic Code\n",
    "\n",
    "Thanks to the CuPy team's effort in creating a 1-to-1 mapping of the NumPy APIs, one of the key advantages is the ability to write the same function and use it interchangeably on either the CPU or GPU.\n",
    "\n",
    "CuPy also provides the capability to identify array types, enabling us to write device-agnostic code. This means we can pass an array (whether it's a NumPy or CuPy array) to our custom function and let the interpreter decide which backend to use, based on the location of the array (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agnostic function implementation \n",
    "def softplus(x):\n",
    "    # Infer whether the array is a NumPy or CuPy array\n",
    "    # `xp` will be set to `cp` if x is on the GPU, or `np` if x is on the CPU\n",
    "    xp = cp.get_array_module(x)\n",
    "    print(\"Using:\", xp.__name__)  # Display the library in use\n",
    "    return xp.maximum(0, x) + xp.log1p(xp.exp(-abs(x)))  # Softplus computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NumPy array on the CPU and transfer it to the GPU\n",
    "x_cpu = np.random.random(10_000)  # Generate a random array with 10,000 elements\n",
    "x_gpu = cp.asarray(x_cpu)  # Transfer the NumPy array to the GPU as a CuPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the softplus function on the NumPy array (CPU)\n",
    "result_cpu = softplus(x_cpu)  # Compute softplus for the CPU array\n",
    "result_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the softplus function on the CuPy array (GPU)\n",
    "result_gpu = softplus(x_gpu)  # Compute softplus for the GPU array\n",
    "result_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Benchmarking\n",
    "\n",
    "CuPy includes a built-in profiler that facilitates the creation and management of all `cuda.Event` objects necessary for measuring execution time on the device. This allows you to easily benchmark the performance of your GPU computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CuPy profiler for benchmarking\n",
    "# (`cupyx` is the module including the cupy-specific functionalities)\n",
    "from cupyx.profiler import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the softplus function on the CPU\n",
    "cpu_bench = benchmark(softplus, (x_cpu,), n_repeat=10)  # Repeat the benchmark 10 times\n",
    "print(cpu_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the softplus function on the GPU\n",
    "gpu_bench = benchmark(softplus, (x_gpu,), n_repeat=10)  # Repeat the benchmark 10 times\n",
    "print(gpu_bench)  # Display the benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Kernels\n",
    "\n",
    "CuPy provides three types of CUDA kernel definitions:\n",
    "- Elementwise\n",
    "- Reduction\n",
    "- Raw\n",
    "\n",
    "### Elementwise Kernels\n",
    "\n",
    "Elementwise kernels are functions or operations that are applied independently to each element of one or more input arrays. These operations are executed simultaneously in parallel across multiple data elements.\n",
    "\n",
    "This concept is similar to the `vectorized` or `guvectorized` functions we've encountered with Numba and CUDA.\n",
    "\n",
    "The definition of an elementwise kernel consists of four parts:\n",
    "1. An input argument list.\n",
    "2. An output argument list.\n",
    "3. The kernel body code.\n",
    "4. The kernel name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an elementwise kernel using CuPy\n",
    "kernel = cp.ElementwiseKernel(\n",
    "    'float32 x, float32 y',             # Input argument list\n",
    "    'float32 z',                        # Output argument list\n",
    "    '''if (x - 2 > y) { z = x * y; }     \n",
    "    else { z = x + y; }''',             # Kernel body code (executed on each thread)\n",
    "    'elemwise_kernel'                   # Name of the kernel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two CuPy arrays for kernel input\n",
    "x = cp.arange(6, dtype='float32').reshape(2, 3)  # 2x3 array with float32 data type\n",
    "y = cp.arange(3, dtype='float32')                # 1D array with 3 elements and float32 data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the elementwise kernel using CuPy\n",
    "kernel_bench = benchmark(kernel, (x, y), n_repeat=10)  # Repeat the benchmark 10 times\n",
    "print(kernel_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the elementwise kernel on the input arrays and retrieve the result\n",
    "z = kernel(x, y)  # Apply the kernel\n",
    "result = z.get()  # Transfer the result from GPU to CPU\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same kernel, which is currently defined to operate specifically on `float32` arrays, can be made generic to support arbitrary data types. This allows the data type to be determined at compile time, enabling greater flexibility and reusability of the kernel for different types of input arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic type elementwise kernel using CuPy\n",
    "kernel_gtype = cp.ElementwiseKernel(\n",
    "    'T x, T y',                          # Input arguments of type T\n",
    "    'T z',                               # Output argument of the same type T\n",
    "    '''if (x - 2 > y) { z = x * y; }     \n",
    "    else { z = x + y; }''',              \n",
    "    'elemwise_kernel_generic_type'      # Name of the kernel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two CuPy arrays with integer data types and execute the generic type kernel\n",
    "x = cp.arange(6, dtype='int32').reshape(2, 3)  # 2x3 array of int32\n",
    "y = cp.arange(3, dtype='int32')                 # 1D array of int32\n",
    "\n",
    "# Apply the generic type kernel\n",
    "z = kernel_gtype(x, y)  \n",
    "result_generic = z.get()  # Transfer the result from GPU to CPU\n",
    "result_generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Kernels\n",
    "\n",
    "Reduction kernels are functions or operations that combine multiple elements of an input array into a single result by applying a reduction operation, such as summing, finding the minimum or maximum, or calculating the average of the elements. These kernels are particularly useful for efficiently computing global aggregates or statistics from large arrays.\n",
    "\n",
    "The definition of a reduction kernel consists of several components:\n",
    "1. **Identity Value**: This value serves as the initial value for the reduction process.\n",
    "2. **Mapping Expression**: This is used for the pre-processing of each element to be reduced.\n",
    "3. **Reduction Expression**: This operator reduces multiple mapped values, using the special variables `a` and `b` as its operands.\n",
    "4. **Post Mapping Expression**: This transforms the resulting reduced values, using the special variable `a` as input. The output should be written to the output parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reduction kernel for computing the L2 norm using CuPy\n",
    "l2norm_kernel = cp.ReductionKernel(\n",
    "    'T x',              # Input parameters\n",
    "    'T y',              # Output parameters\n",
    "    'x * x',            # Mapping expression: squares each element\n",
    "    'a + b',            # Reduction expression: sums the mapped values\n",
    "    'y = sqrt(a)',      # Post-reduction mapping: takes the square root of the sum\n",
    "    '0',                # Identity value for the reduction\n",
    "    'l2norm'            # Name of the kernel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CuPy array and compute the L2 norm using the reduction kernel\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)  # 2x5 array of float32\n",
    "l2norm_result = l2norm_kernel(x, axis=1)  # Compute L2 norm along axis 1\n",
    "l2norm_result_host = l2norm_result.get()\n",
    "l2norm_result_host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Kernels (The Good Old CUDA-C Kernels...)\n",
    "\n",
    "With raw kernels, we can define kernels directly from raw CUDA source code.\n",
    "\n",
    "Much like Numba+CUDA, this approach bridges the gap between Python and CUDA-C, allowing us to leverage the exact CUDA-C functions we've previously declared in our Python code.\n",
    "\n",
    "This means we can build the majority of our codebase using Python and NumPy, accelerating specific parts as needed by using CuPy as a drop-in replacement. We can then call CUDA-C functions—and even external CUDA-C libraries—while remaining within the same Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a raw CUDA kernel for matrix multiplication using CuPy\n",
    "custom_kernel = cp.RawKernel(r'''\n",
    "    extern \"C\" __global__ \n",
    "    void naiveMatrixMultiplication(const float* M, const float* N, float* P, const int width) {\n",
    "        // Calculate the thread ID within the overall grid\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        // Each thread computes one element of the result matrix\n",
    "        if (row < width && col < width) {\n",
    "            float sum = 0.0;\n",
    "            // Access all elements of a row of M and a column of N\n",
    "            for (int k = 0; k < width; ++k) {\n",
    "                sum += M[row * width + k] * N[k * width + col];\n",
    "            }\n",
    "            P[row * width + col] = sum;  // Store the result\n",
    "        }\n",
    "    }\n",
    "    ''',\n",
    "    'naiveMatrixMultiplication')  # Name of the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimensions for the matrices and create them\n",
    "width = 2048  # Size of the square matrices\n",
    "M = cp.random.random((width, width), 'float32')  # Random matrix M of size 2048x2048\n",
    "N = cp.random.random((width, width), 'float32')  # Random matrix N of size 2048x2048\n",
    "P = cp.zeros_like(N)  # Result matrix P initialized to zeros with the same shape as N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of threads per block and calculate the number of blocks needed\n",
    "threads_per_block = (32, 32)  # Each block will have 32x32 threads\n",
    "blocks = (int((width + threads_per_block[0] - 1) / threads_per_block[0]), \n",
    "           int((width + threads_per_block[1] - 1) / threads_per_block[1]))  # Calculate blocks needed\n",
    "\n",
    "print(\"Threads per block:\", threads_per_block)  # Output the thread configuration\n",
    "print(\"Number of blocks:\", blocks)  # Output the calculated number of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the kernel\n",
    "custom_kernel(blocks, threads_per_block, (M, N, P, width))\n",
    "# Retrieve the P values\n",
    "P.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the raw CUDA kernel for matrix multiplication\n",
    "kernel_bench = benchmark(custom_kernel, (blocks, threads_per_block, (M, N, P, width)), n_repeat=5)  # Repeat the benchmark 5 times\n",
    "print(kernel_bench)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing P will result in a call to the device memory location (an implicit `get`)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for matrix multiplication using CuPy's dot product\n",
    "def cp_matmul(M, N):\n",
    "    return cp.dot(M, N)  # Multiply matrices M and N using CuPy's dot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the CuPy matrix multiplication function\n",
    "cp_matmul_bench = benchmark(cp_matmul, (M, N), n_repeat=5)  # Repeat the benchmark 5 times\n",
    "print(cp_matmul_bench)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matrix multiplication using the CuPy-defined function\n",
    "P_cp = cp_matmul(M, N)  # Multiply matrices M and N, storing the result in P_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the result from CuPy's matrix multiplication is close to the expected result\n",
    "is_close = np.allclose(P, P_cp)  # Verify that all elements in P and P_cp are close within a tolerance\n",
    "is_close  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Algebraic and Scientific Applications Made Simple\n",
    "\n",
    "Most of the algebraic functionalities from NumPy, as well as some from SciPy (though not all; please check the documentation for details), are included in CuPy's library of rewritten CUDA kernels that operate on CuPy inputs.\n",
    "\n",
    "* [Reference of NumPy routines included in CuPy](https://docs.cupy.dev/en/stable/reference/routines.html)\n",
    "* [Reference of SciPy routines included in CuPy](https://docs.cupy.dev/en/stable/reference/scipy.html)\n",
    "* [An extremely useful comparison between NumPy and CuPy](https://docs.cupy.dev/en/stable/reference/comparison.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform singular value decomposition (SVD) using NumPy\n",
    "x_cpu = np.random.random((1000, 1000))  # Generate a random 1000x1000 matrix\n",
    "u, s, v = np.linalg.svd(x_cpu)  # Compute the SVD of the matrix, resulting in U, singular values S, and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform singular value decomposition (SVD) using CuPy\n",
    "x_gpu = cp.asarray(x_cpu)  # Use the same inputs as before\n",
    "u_cp, s_cp, v_cp = cp.linalg.svd(x_gpu)  # Compute the SVD of the matrix on the GPU, resulting in U, singular values S, and V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy data for polynomial fitting\n",
    "x = np.linspace(0, 10, 100)  # Create an array of 100 points from 0 to 10\n",
    "y_true = 2 * np.sin(x) + 0.5 * x  # Define the true relationship\n",
    "noise = np.random.normal(0, 0.5, x.shape)  # Generate Gaussian noise\n",
    "y = y_true + noise  # Create the noisy observations\n",
    "\n",
    "# Perform polynomial fitting using numpy.polyfit()\n",
    "degree = 5  # Define the degree of the polynomial\n",
    "coeffs = np.polyfit(x, y, degree)  # Fit a polynomial of the specified degree to the data\n",
    "\n",
    "# Generate predictions using the fitted polynomial\n",
    "y_pred = np.polyval(coeffs, x)  # Evaluate the polynomial at the points in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the noisy data and the polynomial fit\n",
    "plt.plot(x, y, '.', label='Data')  # Scatter plot of the noisy data\n",
    "plt.plot(x, y_pred, '-', label='Fit')  # Line plot of the fitted polynomial\n",
    "plt.legend(loc='best')  # Display the legend in the best location\n",
    "plt.xlabel('X')  # Label for the x-axis\n",
    "plt.ylabel('Y')  # Label for the y-axis\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy data on the GPU using CuPy\n",
    "x_gpu = cp.asarray(x)  # Convert the x array to a CuPy array\n",
    "y_gpu = cp.asarray(y)  # Convert the y array to a CuPy array\n",
    "\n",
    "# Perform polynomial fitting using cupy.polyfit()\n",
    "degree = 5  # Define the degree of the polynomial\n",
    "coeffs_cp = cp.polyfit(x_gpu, y_gpu, degree)  # Fit a polynomial of the specified degree to the data on the GPU\n",
    "\n",
    "# Generate predictions using the fitted polynomial\n",
    "y_pred_cp = cp.polyval(coeffs_cp, x_gpu)  # Evaluate the polynomial at the points in x using CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the noisy data and the polynomial fit\n",
    "plt.plot(x, y, '.', label='Data')  # Scatter plot of the noisy data\n",
    "plt.plot(x, y_pred, '.-', label='Fit (CPU)')  # Line plot of the fitted polynomial (CPU)\n",
    "plt.plot(x, y_pred_cp.get(), '--', label='Fit (GPU)')  # Line plot of the fitted polynomial (GPU)\n",
    "plt.legend(loc='best')  # Display the legend in the best location\n",
    "plt.xlabel('X')  # Label for the x-axis\n",
    "plt.ylabel('Y')  # Label for the y-axis\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciPy-equivalent functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image with repeated delta functions\n",
    "deltas = np.zeros((2048, 2048))  # Initialize a 2048x2048 array filled with zeros\n",
    "deltas[8::16, 8::16] = 1  # Set every 16th pixel starting from (8, 8) to 1, creating a grid of delta functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a zoomed-in version of the grid of delta functions\n",
    "plt.imshow(deltas[0:200, 0:200])  # Display a 200x200 section of the array\n",
    "plt.colorbar()  # Add a color bar to the side for reference\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian filter\n",
    "x, y = np.meshgrid(np.linspace(-2, 2, 15), np.linspace(-2, 2, 15))  # Create a mesh grid\n",
    "dst = np.sqrt(x * x + y * y)  # Calculate the distance from the center\n",
    "sigma = 1.0  # Standard deviation for the Gaussian\n",
    "muu = 0.0  # Mean (center) of the Gaussian\n",
    "gauss = np.exp(-((dst - muu) ** 2 / (2.0 * sigma ** 2)))  # Compute the Gaussian function\n",
    "\n",
    "# Plot the Gaussian filter\n",
    "plt.imshow(gauss)  # Display the Gaussian filter with a color map\n",
    "plt.colorbar()  # Add a color bar to the side for reference\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the delta functions and Gaussian filter to the GPU using CuPy\n",
    "deltas_gpu = cp.array(deltas)  # Convert the delta functions array to a CuPy array on the GPU\n",
    "gauss_gpu = cp.array(gauss)  # Convert the Gaussian filter array to a CuPy array on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the equivalent CuPy function for 2D convolution\n",
    "# (Equivalent in CuPy to the host code --> from scipy.signal import convolve2d)\n",
    "from cupyx.scipy.signal import convolve2d  # Import the 2D convolution function from CuPy's SciPy module\n",
    "\n",
    "# Apply the 2D convolution\n",
    "convolved_img_gpu = convolve2d(deltas_gpu, gauss_gpu)  # Convolve the delta functions with the Gaussian filter on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the convolved image result from the GPU to the host (CPU)\n",
    "convolved_img = convolved_img_gpu.get()  # Retrieve the convolved image as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a zoomed-in version of the smeared grid after convolution\n",
    "plt.imshow(convolved_img[0:200, 0:200])  # Display a 200x200 section of the convolved image\n",
    "plt.colorbar()  # Add a color bar to the side for reference\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
