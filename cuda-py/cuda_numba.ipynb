{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA + Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Numba for Pythonic GPU-accelerated Code\n",
    "\n",
    "\"**Numba** is a just-in-time compiler for Python functions.\"\n",
    "\n",
    "In addition to its primary function, Numba supports CUDA GPU programming by directly compiling a **restricted subset of Python code** into CUDA kernels and device functions, following the CUDA execution model.\n",
    "\n",
    "As discussed in class, there are two important points to keep in mind when working with code that runs on GPUs:\n",
    "1. Data required for kernel execution must be transferred between the host and the device\n",
    "2. GPU kernels will not provide performance benefits over CPU code unless the full bandwidth of the GPU is utilized effectively\n",
    "\n",
    "In this notebook, we will explore how to program a GPU using Python, write CUDA kernels, and manage data exchange between the host and device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with what we covered last on Numba: the `@vectorize` and `@guvectorize` decorators. These decorators allow us to create compiled functions that can target multiple execution environments, including `CPU`, multithreaded `parallel` execution, and `CUDA` for GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's use the `numba.cuda` API to detect and list the available CUDA devices. This will help us ensure that we have the necessary hardware for GPU programming and allow us to verify the device's capabilities before we start writing CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, vectorize, guvectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if any CUDA-capable GPU is available by using the `cuda.is_available()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify availability of CUDA\n",
    "cuda_available = cuda.is_available()\n",
    "cuda_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can detect the supported CUDA hardware by using the `cuda.detect()` function. This function will provide a summary of the available CUDA devices and their capabilities, helping us understand the specifications of the hardware we will be using for GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary information of the available CUDA devices\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing ufuncs for the GPU with `@vectorize`\n",
    "\n",
    "Let's redefine a simple `vectorize` universal function, explicitly stating that it will be executed as a kernel on the GPU.\n",
    "\n",
    "It's important to note that, in order to run on a GPU, the CUDA `@vectorize` and `@guvectorize` decorators will not behave identically to the NumPy `ufunc` produced by standard Numba. We need to be aware of these differences as we write GPU-accelerated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vectorized universal function for execution on --CPU--\n",
    "@vectorize(['int64(int64, int64)',\n",
    "            'int32(int32, int32)',\n",
    "            'float64(float64, float64)',\n",
    "            'float32(float32, float32)'], target='cpu') \n",
    "def residual(x, y):\n",
    "    return (y - x) / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vectorized universal function for execution on --GPU--\n",
    "# In this case, the signature and target types are explicitely required\n",
    "@vectorize(['int64(int64, int64)',\n",
    "            'int32(int32, int32)',\n",
    "            'float64(float64, float64)',\n",
    "            'float32(float32, float32)'], target='cuda') \n",
    "def residual_cu(x, y):\n",
    "    return (y - x) / x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs and outputs for the `@vectorize` decorator are specified as a list of signatures, allowing us to compile the same universal function (ufunc) for multiple data types on the GPU.\n",
    "\n",
    "The CUDA `ufunc` also supports passing arrays that are already on the GPU device (we will explore this later). However, it can also accept arrays that reside on the host. \n",
    "\n",
    "Numba automatically manages the transfer of data between the host and the device during the function call, simplifying the programming process for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for array creation\n",
    "import numpy as np\n",
    "\n",
    "# Create two random arrays \n",
    "a = np.random.random(100_000)\n",
    "b = np.random.random(100_000)\n",
    "\n",
    "# Check the data type of the array 'a'\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "\n",
    "# Calculate the residuals using NumPy operations\n",
    "c = (b - a) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "\n",
    "# Calculate the residuals using the CPU implementation\n",
    "c = residual(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "\n",
    "# Calculate the residuals using the GPU implementation\n",
    "c = residual_cu(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple call to the GPU `@vectorize` function, Numba automatically handles several important tasks:\n",
    "\n",
    "- Compiled the CUDA kernel.\n",
    "- (!) Allocated GPU memory for the inputs and outputs.\n",
    "- (!) Copied the input data to the GPU.\n",
    "- (!) Executed the CUDA kernel (GPU function) with the appropriate kernel dimensions based on the input sizes.\n",
    "- (!) Copied the result back from the GPU to the CPU.\n",
    "- Returned the result as a NumPy array on the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a beautifully simple and compact piece of code that performs all the actions we used to execute explicitly in CUDA-C with almost zero effort.\n",
    "\n",
    "***BUT...*** the performance may not meet expectations (i.e. may be crappy).\n",
    "\n",
    "Based on our experience with CUDA-C, there are several potential root causes for subpar performance:\n",
    "\n",
    "- **Input Size**: The inputs may be too small for the GPU, which is designed for throughput rather than low-latency operations. This can lead to insufficient performance given the hardware differences between CPU cores and GPU SPs.\n",
    "  \n",
    "- **Simple Calculations**: The calculation being performed may be too simple. Sending a task to the GPU incurs significant overhead compared to executing a function on the CPU. If our Compute operations to Global Memory Accesses (CGMA) ratio is too low, the latency associated with global memory access can dominate the computation time.\n",
    "\n",
    "- **Data Transfer Overhead**: The `timeit` measurement also includes the time taken to copy data to and from the GPU. While Numba simplifies data management for a single function call, it is often more efficient to run multiple GPU operations sequentially, transferring data to the GPU once and keeping it there until all processing is complete.\n",
    "\n",
    "- **Data Types**: Our data types might not be optimal for the task at hand, potentially being larger than necessary. For example, using `float64` instead of `float32` when precision requirements allow for a smaller data type can lead to inefficiencies (double the size of data moved and accessed).\n",
    "\n",
    "We have extensively discussed that maximizing the performance of our GPU computations requires more than just dispatching the same code to the GPU. It involves rethinking algorithms, estimating the relationship between data size and computational complexity, and implementing careful memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Precompute this constant as a float32\n",
    "SQRT_2PI = np.float32((2 * math.pi) ** 0.5)\n",
    "\n",
    "# Define a vectorized function to compute the values of a Gaussian on the GPU\n",
    "@vectorize(['float32(float32, float32, float32)'], \n",
    "           target='cuda')\n",
    "def gaussian_cu(x, m, s):\n",
    "    return math.exp(-0.5 * ((x - m) / s) ** 2) / (s * SQRT_2PI)\n",
    "\n",
    "# Define a function to compute the values of a Gaussian with NumPy\n",
    "def gaussian_np(x, m, s):\n",
    "    return np.exp(-0.5 * ((x - m) / s) ** 2) / (s * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to evaluate the Gaussian function a few million times with the same mean and standard deviation\n",
    "x = np.random.uniform(-5, 5, size=10_000_000).astype('float32')\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "\n",
    "# Compute the Gaussian values with NumPy\n",
    "gaussian_np(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "\n",
    "# Compute the Gaussian values with Numba+CUDA\n",
    "gaussian_cu(x,mean,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we are observing performance improvements with the following optimizations:\n",
    "\n",
    "- **Using `float32` Type**: Leveraging `float32` instead of `float64` reduces memory usage and increases computation speed.\n",
    "  \n",
    "- **More Computation per Memory Access**: The calculations performed within the GPU function provide a higher computation-to-memory access ratio, which is essential for maximizing performance.\n",
    "\n",
    "- **High Concurrency**: A large number of concurrent GPU threads are active during execution, further enhancing throughput.\n",
    "\n",
    "It's important to note that the timing for the GPU function still includes the overhead associated with copying all the data to and from the GPU. This overhead can impact overall performance, particularly for smaller workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why are we using `math.exp` instead of `np.exp`?\n",
    "> \n",
    "> We use `math.exp` instead of `np.exp` due to the limitations of Numba's CUDA support for NumPy functions. \n",
    "> \n",
    "> Depending on the version of Numba you have installed, you may often need to use scalar Python functions instead of their NumPy equivalents when targeting CUDA. While `np.exp` (and other NumPy functions) may work in more recent or future versions of Numba, current versions may require this specific approach.\n",
    "> \n",
    "> Given our current hardware and software configuration, we are using a specific combination of packages:\n",
    "> ```bash\n",
    "> # Name                    Version                   Build  Channel\n",
    "> python                    3.9.10       h1b383ca_2_cpython    conda-forge\n",
    "> numpy                     1.23.5           py39hf5a3166_0    conda-forge\n",
    "> numba                     0.56.4           py39h6619693_1    conda-forge\n",
    "> ```\n",
    "> \n",
    "> It's worth noting that recent versions of Numba have included compatibility with many more NumPy functions.\n",
    "> \n",
    "> As always, checking the documentation for the specific versions we use it's paramount... [https://numba.readthedocs.io/en/stable/cuda/cudapysupported.html](https://numba.readthedocs.io/en/stable/cuda/cudapysupported.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing ufuncs for the GPU with `@guvectorize`\n",
    "\n",
    "The same considerations apply to functions that operate on entire ndarrays using `@guvectorize`.\n",
    "\n",
    "For example, let's rewrite the matrix element-wise addition code using Numba and CUDA with the `@guvectorize` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the signature for the guvectorize function\n",
    "# and the layouts of the input and output\n",
    "# 2 matrices (A, B) as input and 1 matrix (C) as output\n",
    "# All elements are of type float32\n",
    "@guvectorize('(float32[:,:], float32[:,:], float32[:,:])',\n",
    "             '(x,y),(x,y)->(x,y)', \n",
    "             target='cuda')\n",
    "def matrix_addition(A, B, C):\n",
    "    # Loop over each element of the 2D arrays\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            # Perform element-wise addition\n",
    "            C[i, j] = A[i, j] + B[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the matrices\n",
    "rows, cols = 1024, 1024\n",
    "\n",
    "# Create two random matrices A and B with float32 elements\n",
    "A = np.random.rand(rows, cols).astype(np.float32)\n",
    "B = np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "# Create an empty matrix for the result C\n",
    "C = np.zeros((rows, cols), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "# Perform the matrix addition using the guvectorized function\n",
    "matrix_addition(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting matrix\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that `@guvectorize` does not allow returning the result directly. Instead, the result object must be passed as an input parameter. This design choice emphasizes the need for explicit memory management in GPU programming, ensuring that the output is written to a predefined location in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "So far, we have operated directly with NumPy arrays on the host. \n",
    "\n",
    "During the kernel call, Numba automatically handles the transfer of these arrays to the device and back to the host after execution. While this convenience is useful, it is not particularly efficient.\n",
    "\n",
    "In most cases, you will want to keep data on the GPU and launch multiple kernels without being interrupted by transfers between the device and the host.\n",
    "\n",
    "We can utilize the CUDA APIs for manual data management, allowing for more efficient handling of memory on the GPU:\n",
    "\n",
    "- `cuda.device_array`: Allocates memory on the device.\n",
    "- `cuda.to_device`: Allocates memory on the device and, by default (`copy=True`), copies data from an existing host array.\n",
    "- `cuda.copy_to_host`: Transfers data from the device memory back to the host.\n",
    "\n",
    "Now, let's return to the previous example and see how we can apply these concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the matrices\n",
    "rows, cols = 1024, 1024\n",
    "\n",
    "# Create host arrays A and B with random float32 values\n",
    "A = np.random.rand(rows, cols).astype(np.float32)\n",
    "B = np.random.rand(rows, cols).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if A is a CUDA array\n",
    "# (an array managed by the CUDA backend)\n",
    "cuda.is_cuda_array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device arrays and perform a host-to-device copy\n",
    "# Both operations are done in a single call with `to_device()`\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "\n",
    "# Allocate a device array with the same shape and type as d_A\n",
    "d_C = cuda.device_array_like(d_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if d_A is a CUDA array\n",
    "cuda.is_cuda_array(d_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape and memory allocation details for the device array d_A\n",
    "print(f'Shape of array d_A in device memory: {d_A.shape}')\n",
    "print(f'Bytes allocated for d_A: {d_A.nbytes / 1e6:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "\n",
    "# Perform the matrix addition using the previous guvectorized funtion but with device memory\n",
    "matrix_addition(d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time measurement is now completely bogus!\n",
    "\n",
    "The launch of a kernel is an asynchronous operation that does not block the CPU. As a result, the time we measured does not accurately reflect the execution time of the task on the GPU.\n",
    "\n",
    "To obtain a reliable measurement, we should revert to using CUDA Events for timing. CUDA Events allow us to record timestamps before and after the kernel execution, enabling us to measure the actual GPU execution time more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CUDA events for timing\n",
    "start = cuda.event()\n",
    "stop = cuda.event()\n",
    "\n",
    "# Start the timer\n",
    "start.record()\n",
    "\n",
    "# Perform the matrix addition using the device memory\n",
    "matrix_addition(d_A, d_B, d_C)\n",
    "\n",
    "# Stop the timer and synchronize it\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "\n",
    "# Get the elapsed time between start and stop (in milliseconds)\n",
    "print(f\"Elapsed time: {cuda.event_elapsed_time(start, stop):.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result from the device back to the host\n",
    "C = d_C.copy_to_host()\n",
    "\n",
    "# Display the result\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed that it is very important to free up memory on the device by deallocating the memory used in previous kernel executions.\n",
    "\n",
    "However, it's essential to note that Python is not `C` or `C++`. An analogous call to `cudaFree` will not have an immediate effect when issued.\n",
    "\n",
    "Python is a garbage-collected language, meaning that memory deallocation will be handled by the Python interpreter at some point in the future... eventually...\n",
    "\n",
    "While it is always good practice to deallocate memory, the underlying logic in Python is less straightforward compared to memory-managed languages like `C` or `C++`. Therefore, we must be mindful of how and when we manage device memory to avoid potential memory leaks or unnecessary memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete (free?) the device-allocated arrays\n",
    "del d_A, d_B, d_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Python did with this command is remove the references to the `d_A`, `d_B`, and `d_C` objects, but it does not immediately free up the memory they occupy.\n",
    "\n",
    "Numba will take care of releasing the memory when the garbage collector runs, which may not happen right away.\n",
    "\n",
    "We don't need to know the intricate details of how this works, but it's important to understand that this is one of the reasons why GPU code is typically written in memory-managed languages like `C` or `C++`. In those languages, developers have direct control over memory allocation and deallocation, allowing for more predictable memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressing CUDA Kernels with `@jit`\n",
    "\n",
    "In addition to creating kernels as `@vectorize` NumPy-like functions, we can use `@jit` or `@njit` to express entire functions targeted at the GPU instead of the CPU.\n",
    "\n",
    "Let’s revisit the creation of the Julia fractal, this time rewriting the Numba code to execute as a CUDA kernel.\n",
    "\n",
    "To do this, we need to access the \"location\" of the kernel during execution on the grid. The `numba.cuda` module provides access to these registers, similar to what we see in CUDA-C.\n",
    "\n",
    "For a 1D grid of blocks and threads, you can determine the unique index for each thread using the following code:\n",
    "\n",
    "```python\n",
    "tx = cuda.threadIdx.x\n",
    "bx = cuda.blockIdx.x\n",
    "bw = cuda.blockDim.x\n",
    "idx = tx + bx * bw\n",
    "```\n",
    "\n",
    "This allows you to access each element of your data using:\n",
    "\n",
    "```python\n",
    "array[idx] = ...\n",
    "```\n",
    "\n",
    "Similarly, for a 2D grid of blocks and threads, one can use: \n",
    "```python\n",
    "tx = cuda.threadIdx.x\n",
    "ty = cuda.threadIdx.y\n",
    "\n",
    "bx = cuda.blockIdx.x\n",
    "by = cuda.blockIdx.y\n",
    "\n",
    "bw = cuda.blockDim.x\n",
    "bh = cuda.blockDim.y\n",
    "\n",
    "idx_x = tx + bx * bw\n",
    "idx_y = ty + by * bh\n",
    "\n",
    "[...]\n",
    "\n",
    "array[idx_x, idx_y] = ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Additionally, `numba.cuda` offers a simple API `cuda.grid(ndim)`  access the unique identifier of the thread in a 1D or 2D grid by calling:\n",
    "\n",
    "```python\n",
    "idx = cuda.grid(1)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "idx_x, idx_y = cuda.grid(2)\n",
    "```\n",
    "\n",
    "These functions return the absolute position of the current thread within the entire grid of blocks. The parameter `ndim` should match the number of dimensions specified when instantiating the kernel. If `ndim` is 1, a single integer is returned. If `ndim` is 2 or 3, a tuple containing the respective number of integers is returned.\n",
    "\n",
    "\n",
    "Similarly, we can check the dimension of the grid by means of `numba.cuda.gridsize()`, which will return the absolute size (or shape) in threads of the entire grid of blocks. `ndim` has the same meaning as in `grid()` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-computing the Julia Set with `cuda.jit`\n",
    "\n",
    "Now, let's rewrite the equivalent plain-Numba code that was used to evaluate the Julia set, this time utilizing `cuda.jit` for CUDA execution in Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def julia_fractal(z_re, z_im, j):\n",
    "    # Get the position of the thread in the overall grid\n",
    "    idx_x, idx_y = cuda.grid(2)\n",
    "\n",
    "    \"\"\" This is equivalent to writing \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    idx_x = tx + bx * bw\n",
    "\n",
    "    ty = cuda.threadIdx.y\n",
    "    by = cuda.blockIdx.y\n",
    "    bh = cuda.blockDim.y\n",
    "\n",
    "    idx_y = ty + by * bh\n",
    "    \"\"\"    \n",
    "\n",
    "    # Ensure the thread is within the bounds of the data\n",
    "    if idx_x < j.shape[0] and idx_y < j.shape[1]:\n",
    "        # Initialize the complex number z\n",
    "        z = z_re[idx_x] + 1j * z_im[idx_y]\n",
    "\n",
    "        # Iterate to check the divergence\n",
    "        for t in range(256):\n",
    "            z = z ** 2 - 0.05 + 0.68j\n",
    "            if (z.real * z.real + z.imag * z.imag) > 4.0:\n",
    "                j[idx_x, idx_y] = t\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid dimensions; in this case, a square NxN\n",
    "N = 1024\n",
    "width, height = np.int32(N), np.int32(N)\n",
    "\n",
    "# Create the arrays for the real and imaginary parts\n",
    "z_real = np.linspace(-1.5, 1.5, width).astype('float32')\n",
    "z_imag = np.linspace(-1.5, 1.5, height).astype('float32')\n",
    "\n",
    "# Prepare the output array \n",
    "# Using uint8 since we only need to store values between 0 and 255\n",
    "j = np.zeros((width, height), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device arrays and perform a host-to-device copy (all at once) \n",
    "d_z_real = cuda.to_device(z_real)\n",
    "d_z_imag = cuda.to_device(z_imag)\n",
    "d_j      = cuda.to_device(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CUDA grid dimensions\n",
    "threads_per_block = (8, 8)  # 8x8 threads per block\n",
    "blocks_per_grid_x = math.ceil(j.shape[0] / threads_per_block[0])\n",
    "blocks_per_grid_y = math.ceil(j.shape[1] / threads_per_block[1])\n",
    "\n",
    "# Alternatively, you can use:\n",
    "# blocks_per_grid_x = (j.shape[0] + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "# blocks_per_grid_y = (j.shape[1] + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CUDA events\n",
    "start = cuda.event()\n",
    "stop = cuda.event()\n",
    "\n",
    "# Start the timer\n",
    "start.record()\n",
    "\n",
    "# Launch the CUDA kernel\n",
    "julia_fractal[blocks_per_grid, threads_per_block](d_z_real, d_z_imag, d_j)\n",
    "\n",
    "# Stop the timer and synchronize it\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "\n",
    "# Get the elapsed time between start and stop (in milliseconds)\n",
    "print(f\"Elapsed time: {cuda.event_elapsed_time(start, stop):.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result back to the host, storing it in the existing array j\n",
    "d_j.copy_to_host(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the Julia set \n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(j, cmap=plt.cm.RdBu_r, extent=[-1.5, 1.5, -1.5, 1.5])\n",
    "ax.set_xlabel(\"$\\\\mathrm{Re}(z)$\", fontsize=18)\n",
    "ax.set_ylabel(\"$\\\\mathrm{Im}(z)$\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to free up the memory on the device\n",
    "del d_z_real, d_z_imag, d_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory and Thread Synchronization in Numba CUDA Kernels\n",
    "\n",
    "As we discussed in CUDA-C, optimization of CUDA kernel execution can be achieved by reducing latency associated with global memory access and instead utilizing shared memory. Shared memory is located locally within the Streaming Multiprocessors (SMPs) and can be accessed by all threads within the same block.\n",
    "\n",
    "Numba provides the capability to express the use of shared memory in CUDA by using:\n",
    "\n",
    "```python\n",
    "cuda.shared.array(shape, type)\n",
    "```\n",
    "\n",
    "This call should be placed inside the `cuda.jit` kernel, where memory coalescing is beneficial.\n",
    "\n",
    "To synchronize the shared memory across all threads in a block, it is essential to include appropriate calls to `cuda.syncthreads()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite the _square-matrix multiplication with tiling_ CUDA-C example in Python with Numba+CUDA.\n",
    "\n",
    "For the sake of comparing and porting CUDA-C code to Python, we'll work with the previous CUDA-C example which is limited to only allow square matrices with a width which is proportional to the tile.\n",
    "\n",
    "```c\n",
    "#define WIDTH 2048                      \n",
    "#define TILE_WIDTH 32                   \n",
    "#define THREADS_PER_BLOCK_X TILE_WIDTH  \n",
    "#define THREADS_PER_BLOCK_Y TILE_WIDTH  \n",
    "\n",
    "[...]\n",
    "\n",
    "\n",
    "__global__ void matrixMultiplication(const float* M, const float* N, float* P, const int width) {\n",
    "    __shared__ float M_tile[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float N_tile[TILE_WIDTH][TILE_WIDTH];\n",
    "    \n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = blockIdx.y * TILE_WIDTH + ty;\n",
    "    int col = blockIdx.x * TILE_WIDTH + tx;\n",
    "\n",
    "    float sum = 0.;\n",
    "\n",
    "    // Fill the shared memory\n",
    "    // Loop over the tiles of the input matrices\n",
    "    for (int t = 0; t < width / TILE_WIDTH; ++t) {\n",
    "        if ( (row < width) && (t * TILE_WIDTH + tx < width) )\n",
    "            M_tile[ty][tx] = M[row * width + t * TILE_WIDTH + tx];\n",
    "        else \n",
    "            M_tile[ty][tx] = 0.;\n",
    "\n",
    "        if ( (t * TILE_WIDTH + ty < width) && (col < width) )\n",
    "            N_tile[ty][tx] = N[(t * TILE_WIDTH + ty) * width + col];\n",
    "        else \n",
    "            N_tile[ty][tx] = 0.;\n",
    "\n",
    "        // Synchronize (ensure the tile is loaded in shared memory)\n",
    "        __syncthreads();\n",
    "    \n",
    "        // Perform the multiplication for this tile\n",
    "        for (int k = 0; k < TILE_WIDTH; ++k) {\n",
    "            sum += M_tile[ty][k] * N_tile[k][tx];\n",
    "        }\n",
    "\n",
    "        // Ensure all threads are done computing before loading the next tile\n",
    "        __syncthreads(); \n",
    "    }\n",
    "\n",
    "    // Write the result back to the global memory\n",
    "    if (row < width && col < width) {\n",
    "        P[row * width + col] = sum;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for matrix dimensions and tile size\n",
    "WIDTH = 2048 * 2                      \n",
    "TILE_WIDTH = 32                   \n",
    "\n",
    "# Set number of threads per block in both dimensions\n",
    "THREADS_PER_BLOCK_X = TILE_WIDTH  \n",
    "THREADS_PER_BLOCK_Y = TILE_WIDTH  \n",
    "\n",
    "@cuda.jit\n",
    "def matrix_multiplication(M, N, P):\n",
    "    # Allocate shared memory for tiles\n",
    "    M_tile = cuda.shared.array(shape=(TILE_WIDTH, TILE_WIDTH), dtype='float32')\n",
    "    N_tile = cuda.shared.array(shape=(TILE_WIDTH, TILE_WIDTH), dtype='float32')\n",
    "\n",
    "    # Thread index within the block\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "\n",
    "    # Calculate the row and column indices for the global matrix\n",
    "    row = cuda.blockIdx.y * TILE_WIDTH + ty\n",
    "    col = cuda.blockIdx.x * TILE_WIDTH + tx\n",
    "\n",
    "    # Initialize sum for the resulting matrix element\n",
    "    sum = np.float32(0.)\n",
    "\n",
    "    # Loop over the tiles of the input matrices\n",
    "    for t in range(M.shape[0] // TILE_WIDTH):\n",
    "        # Load tile from matrix M into shared memory\n",
    "        if (row < WIDTH) and (t * TILE_WIDTH + tx < WIDTH):\n",
    "            M_tile[ty][tx] = M[row][t * TILE_WIDTH + tx]\n",
    "        else:\n",
    "            M_tile[ty][tx] = 0.\n",
    "\n",
    "        # Load tile from matrix N into shared memory\n",
    "        if (t * TILE_WIDTH + ty < WIDTH) and (col < WIDTH):\n",
    "            N_tile[ty][tx] = N[t * TILE_WIDTH + ty][col]\n",
    "        else:\n",
    "            N_tile[ty][tx] = 0.\n",
    "\n",
    "        # Synchronize threads to ensure all data is loaded\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Compute partial sum for the current tile\n",
    "        for k in range(TILE_WIDTH):\n",
    "            sum += M_tile[ty][k] * N_tile[k][tx]\n",
    "        \n",
    "        # Synchronize threads before loading the next tile\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Write the result to the output matrix P\n",
    "    if (row < WIDTH) and (col < WIDTH):\n",
    "        P[row][col] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output matrices\n",
    "A = np.random.random(size=(WIDTH, WIDTH)).astype('float32')  \n",
    "B = np.random.random(size=(WIDTH, WIDTH)).astype('float32')  \n",
    "C = np.zeros_like(A)  # Output matrix C initialized to zero, same shape as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate and transfer data to the device\n",
    "d_A = cuda.to_device(A)  # Copy input matrix A to device memory\n",
    "d_B = cuda.to_device(B)  # Copy input matrix B to device memory\n",
    "d_C = cuda.device_array_like(d_A)  # Allocate device array for output matrix C, same shape as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the grid\n",
    "threads_per_block = (THREADS_PER_BLOCK_X, THREADS_PER_BLOCK_Y)  # Threads per block in x and y dimensions\n",
    "\n",
    "# Calculate the number of blocks needed in each dimension\n",
    "blocks_per_grid_x = math.ceil(d_C.shape[0] / threads_per_block[0])  # Number of blocks along the x dimension\n",
    "blocks_per_grid_y = math.ceil(d_C.shape[1] / threads_per_block[1])  # Number of blocks along the y dimension\n",
    "\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)  # Tuple containing total blocks per grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CUDA events to measure execution time\n",
    "start = cuda.event()  # Create start event\n",
    "stop = cuda.event()   # Create stop event\n",
    "\n",
    "# Record the start time\n",
    "start.record()\n",
    "\n",
    "# Launch the CUDA kernel for matrix multiplication\n",
    "matrix_multiplication[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n",
    "\n",
    "# Record the stop time and synchronize\n",
    "stop.record()  \n",
    "stop.synchronize()  # Wait for the kernel to complete\n",
    "\n",
    "# Calculate and print the elapsed time between start and stop (in milliseconds)\n",
    "elapsed_time = cuda.event_elapsed_time(start, stop)\n",
    "print(f\"Elapsed time: {elapsed_time:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result matrix from device to host\n",
    "C = d_C.copy_to_host()  \n",
    "\n",
    "# Display the resulting matrix\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the optimizations we’ve implemented so far, it's important to acknowledge that with this relatively small amount of data, the CUDA implementation of the function may still be less effective in achieving significant speedups compared to the straightforward NumPy implementation. \n",
    "\n",
    "CUDA excels in handling larger datasets where the overhead of kernel launches and memory transfers can be offset by the parallel processing capabilities of the GPU. However, for small matrices, the additional overhead may outweigh the benefits of parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty NumPy array for the output, with the same shape as matrix A\n",
    "C_np = np.empty_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3  \n",
    "\n",
    "# Perform matrix multiplication using NumPy's matmul function\n",
    "np.matmul(A, B, C_np) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the device memory for deletion\n",
    "del d_A, d_B, d_C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
